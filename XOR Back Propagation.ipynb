{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer XOR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement:\n",
    "\n",
    "Using Backpropagation algorithm to train a two layer **XOR** problem.\n",
    "Let us define a one hidden layer network with two input units, N hidden layer\n",
    "units and one output unit, with training set of D data samples, using the following\n",
    "notation:\n",
    "\n",
    "\n",
    "1. The input vector for two units as $x^{d} = (x^{d}_{1}, x^{d}_{2})$ or $x^{d}_{i}$; i = 1, 2, d = 1, .. . , D. (For the XOR problem introduced below, 4 data points exist in the dataset $x_{1}= (1, 0), x_{2}= (1, 1), x_{3} = (0, 1), x_{4} = (0, 0).)$\n",
    "     \n",
    "     \n",
    "2. the hidden layer with N units as $h_{j}$; j = 1, . . . N. Each of these units (orneurons) is connected to all the units of the previous layer with a weight $w^{1}_{ji}$. Here j refers to the unit index of the hidden layer and the i refers to the unit index of the previous layer.\n",
    "\n",
    "\n",
    "3. the non-linear function ReLu(t) = max(0, t).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Units Number\n",
    "HUN = 2\n",
    "\n",
    "# Input Units Number\n",
    "IUN = 2\n",
    "\n",
    "# Output Units Number\n",
    "OUN = 1\n",
    "\n",
    "segma = 0.5\n",
    "\n",
    "# The speed of descent/learning\n",
    "eta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(t) :\n",
    "    return (t > 0) * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Calculations:\n",
    "The output is then computed as follows:\n",
    "\n",
    "$$y(x^d​, w^1​, w^2​, b^2​, b^3​) = ReLu( \\sum_{j=1}^{N} [w^{2}_{j} Relu( b^{2}_{j} + \\sum_{i= 1}^{2} w^{1}_{ji} x^{d}_{i}) + b_{3}])$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X) :\n",
    "    global W1, W2, B2, B3\n",
    "    return ReLu(B3 + W2 @ ReLu(B2 + W1 @ X.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions:\n",
    "The loss function we use for training in this homework is:\n",
    "$$E_λ​( w^1​, w^2​, b^2​, b^3​)= \\frac{1}{D} \\sum_{d = 1}^{D}​E^d_λ(w^1​, w^2​, b^2​, b^3​)$$\n",
    "Where:\n",
    "$$E_λ ( w^1​, w^2​, b^2​, b^3​)) = (y(x^d​, w^1​, w^2​, b^2​, b^3​) − t^d​)^2​+ λ (|w^1​|^2​+ |w^2​|^2​+|b^2​|^2 + |b^2​|^2​)$$\n",
    "and λ is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss (estimated_y, exact_y, norm = np.linalg.norm) :\n",
    "    global W1, W2, B2, B3\n",
    "    return (estimated_y - exact_y)**2 + segma * ((norm(W1))**2 + (norm(W2))**2 + (norm(B2))**2 + (norm(B3))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "The general formula for updating the weights and bias is via gradient descent, where the steps are indexed by τ = 1, . . . , T, where T is the stopping criteria, another hyperparameter to be estimated.\n",
    "- $w^{1,2}(τ + 1)= w^{1,2}(τ) − η​\\sum^{D}_{d=1}​\\frac{\\partial E}{\\partial w^{1,2} (τ)}$\n",
    "- $b^{2\\,3}​(τ + 1) = b^{2\\,3}​(τ) − η​\\sum^D_{d=1}​\\frac{\\partial{E}}{\\partial b^{2\\,3} (τ)}$\n",
    "\n",
    "where η, the speed of descent/learning is a hyper-parameter to be estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing these gradients:\n",
    "In order to compute these formulae more precisely, we will need the derivative of the Relu(t) function, which can be readily derived\n",
    "\n",
    "$\\frac{\\partial Rule(t)}{\\partial t}$ = (1 if t ≥ 0) and (0 if t < 0)\n",
    "\n",
    "##### ● Top Layer​:\n",
    "$\\frac{\\partial E}{\\partial w^2}​= 2λw^2​+ [ 2(y − t^d​)h $ (if y > 0 else 0)] where h is the output of the hidden layer.\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b^3} = 2λb^3​+ [ 2(y − t^d​)$ (if y > 0 else 0)]\n",
    "\n",
    "##### ● Bottom Layer:\n",
    "$\\frac{\\partial E}{\\partial w^1}​= 2λw^1​+ [ 2(y − t^d​)w^2​x^d​$(if y > 0 else 0)]\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b^2}= 2λb^2​+ [ 2(y − t^d​)w^2​$(if y > 0 else 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(estimated_y, exact_y, X) :\n",
    "    global W1, W2, B2, B3\n",
    "    condition_flag = estimated_y > 0\n",
    "    W1 -= eta * (2 * segma * W1 + (2 * (estimated_y - exact_y) * (W2.T @ X))) * condition_flag\n",
    "    W2 -= eta * (2 * segma * W2 + (2 * (estimated_y - exact_y) * (ReLu(B2 + W1 @ X.T)).T)) * condition_flag\n",
    "    B2 -= eta * (2 * segma * B2 + (2 * (estimated_y - exact_y) * W2.T)) * condition_flag\n",
    "    B3 -= eta * (2 * segma * B3 + (2 * (estimated_y - exact_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algoritm:\n",
    "- Let N=2, λ=0.5, η=0.1 \n",
    "\n",
    "- The back propagation algorithm works by first (at step τ = 0) initializing all the weights and bias at small random values (with zero mean). Then we loop for τ = 1, . . . T and each step we loop for the training data d = 1, . . . , D and for each training pair $(x^d , t^d )$  compute the gradient for each of the parameters (weights and bias).\n",
    "\n",
    "- Plot a graph of the average loss $E_λ​(w^1​, w^2​, b^2​)$ versus τ.\n",
    "- These plots will help you choose the hyper-parameter T, the stoppage step where the loss does not decrease.\n",
    "- Report the final T and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_function(X, exact_y, precision) :\n",
    "    global W1, W2, B2, B3\n",
    "    D = len(exact_y)\n",
    "    previous_loss = sys.maxsize\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    steps = 1\n",
    "    average_loss = 0\n",
    "    while True:\n",
    "        for i in range(D):\n",
    "            x = (X[i])[None, :]\n",
    "            estimated_y = forward(x)\n",
    "            current_loss = loss(estimated_y, exact_y[i]).sum()\n",
    "            gradient(estimated_y, exact_y[i], x)\n",
    "            average_loss += current_loss\n",
    "        average_loss /= D\n",
    "        x_points.append(steps)\n",
    "        y_points.append(average_loss.sum())  \n",
    "        if previous_loss - average_loss <= precision: break\n",
    "        previous_loss = average_loss\n",
    "        average_loss = 0\n",
    "        steps += 1\n",
    "    print(\"Loss reached:{} after T = {} iterations\".format(average_loss, steps))\n",
    "    plt.plot(x_points, y_points)\n",
    "    plt.xlabel('T')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss reached:0.3904934818452397 after T = 18 iterations\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xd = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "W1 = np.random.randn(HUN, IUN)\n",
    "W2 = np.random.randn(OUN, HUN)\n",
    "B2 = np.random.randn(HUN, OUN)\n",
    "B3 = np.random.randn(OUN, OUN)\n",
    "\n",
    "Y = np.array([0, 1, 1, 0])\n",
    "network_function(Xd, Y, 0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
